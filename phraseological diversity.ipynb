{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3220477a-18e4-449c-a2dd-75ed093addbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "76db371c-910e-4054-98f4-2baf510f5310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words in the folder is: 50774\n"
     ]
    }
   ],
   "source": [
    "# word calculator \n",
    "import os\n",
    "\n",
    "def count_words_in_folder(folder_path):\n",
    "    total_words = 0\n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):  # Make sure to process only text files\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                contents = file.read()\n",
    "                total_words += len(contents.split())  # Split the file into words and count them\n",
    "    return total_words\n",
    "\n",
    "# Replace 'your/folder/path' with the path to your folder\n",
    "folder_path = '/home/user/Corpus linguistics final project/Chinese learner corpus/WE_CHN_B1_2_N210'\n",
    "word_count = count_words_in_folder(folder_path)\n",
    "print(f\"The total number of words in the folder is: {word_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210524bf-a08d-440e-8fba-7811c4feda0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phraseological diversity metrics written to SM_phraseological_diversity_A2.csv\n"
     ]
    }
   ],
   "source": [
    "# lemma, POS tag, parse and calculate RTTR based on RTTR function\n",
    "import os\n",
    "import csv\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "\n",
    "# Function to calculate root Type-Token Ratio (RTTR)\n",
    "def calculate_rttr(types):\n",
    "    total_tokens = sum(types.values()) # Total number of occurrences of dependency types (T)\n",
    "    unique_types = len(types)          # Number of unique dependency types (N)\n",
    "    return total_tokens / math.sqrt(unique_types) if unique_types > 0 else 0\n",
    "\n",
    "\n",
    "# Initialize SpaCy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Path to the folder containing text files\n",
    "folder_path = \"/Users/shuyuan/Desktop/CSSMA-master/Chinese spoken corpus/ICNALE_SM/ICNALE_SM_CHN_A2_0_N056\"\n",
    "\n",
    "# CSV output file\n",
    "output_csv = \"SM_phraseological_diversity_A2.csv\"\n",
    "\n",
    "# Header for the CSV file\n",
    "csv_header = ['Filename', 'AMOD TTR', 'ADVMOD TTR', 'DOBJ TTR']\n",
    "\n",
    "# Process each text file in the folder\n",
    "results = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):  # Ensure we're only processing .txt files\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Process the text with SpaCy\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Collect tokens for each relation\n",
    "        relations = {'amod': Counter(), 'advmod': Counter(), 'dobj': Counter()}\n",
    "        for token in doc:\n",
    "            if token.dep_ in relations:\n",
    "                relations[token.dep_].update([(token.head.lemma_, token.lemma_)])\n",
    "\n",
    "        # Calculate TTR for each relation\n",
    "        amod_rttr = calculate_rttr(relations['amod'])\n",
    "        advmod_rttr = calculate_rttr(relations['advmod'])\n",
    "        dobj_rttr = calculate_rttr(relations['dobj'])\n",
    "\n",
    "        # Append the results for this file\n",
    "        results.append([filename, amod_rttr, advmod_rttr, dobj_rttr])\n",
    "\n",
    "# Write the results to a CSV file\n",
    "with open(output_csv, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(csv_header)  # Write the header\n",
    "    writer.writerows(results)  # Write the content\n",
    "\n",
    "print(f\"Phraseological diversity metrics written to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb7295bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phraseological diversity metrics written to SM_phraseological_diversity_B1_2.csv\n"
     ]
    }
   ],
   "source": [
    "# include calculation of the mean and sd\n",
    "import os\n",
    "import csv\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import math\n",
    "import statistics  # Import statistics module for mean and standard deviation calculations\n",
    "\n",
    "# Function to calculate root Type-Token Ratio (RTTR)\n",
    "def calculate_rttr(types):\n",
    "    total_tokens = sum(types.values())  # Total number of occurrences of dependency types (T)\n",
    "    unique_types = len(types)           # Number of unique dependency types (N)\n",
    "    return total_tokens / math.sqrt(unique_types) if unique_types > 0 else 0\n",
    "\n",
    "# Initialize SpaCy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Path to the folder containing text files\n",
    "folder_path = \"/Users/shuyuan/Desktop/CSSMA-master/Chinese spoken corpus/ICNALE_SM/ICNALE_SM_CHN_B1_2_N312\"\n",
    "\n",
    "# CSV output file\n",
    "output_csv = \"SM_phraseological_diversity_B1_2.csv\"\n",
    "\n",
    "# Header for the CSV file\n",
    "csv_header = ['Filename', 'AMOD TTR', 'ADVMOD TTR', 'DOBJ TTR']\n",
    "\n",
    "# Process each text file in the folder\n",
    "results = []\n",
    "amod_ttrs = []\n",
    "advmod_ttrs = []\n",
    "dobj_ttrs = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):  # Ensure we're only processing .txt files\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Process the text with SpaCy\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Collect tokens for each relation\n",
    "        relations = {'amod': Counter(), 'advmod': Counter(), 'dobj': Counter()}\n",
    "        for token in doc:\n",
    "            if token.dep_ in relations:\n",
    "                relations[token.dep_].update([(token.head.lemma_, token.lemma_)])\n",
    "\n",
    "        # Calculate TTR for each relation\n",
    "        amod_rttr = calculate_rttr(relations['amod'])\n",
    "        advmod_rttr = calculate_rttr(relations['advmod'])\n",
    "        dobj_rttr = calculate_rttr(relations['dobj'])\n",
    "\n",
    "        # Append TTR values to their respective lists\n",
    "        amod_ttrs.append(amod_rttr)\n",
    "        advmod_ttrs.append(advmod_rttr)\n",
    "        dobj_ttrs.append(dobj_rttr)\n",
    "\n",
    "        # Append the results for this file\n",
    "        results.append([filename, amod_rttr, advmod_rttr, dobj_rttr])\n",
    "\n",
    "# Calculate mean and standard deviation for each TTR category\n",
    "mean_amod = statistics.mean(amod_ttrs) if amod_ttrs else 0\n",
    "sd_amod = statistics.stdev(amod_ttrs) if len(amod_ttrs) > 1 else 0\n",
    "mean_advmod = statistics.mean(advmod_ttrs) if advmod_ttrs else 0\n",
    "sd_advmod = statistics.stdev(advmod_ttrs) if len(advmod_ttrs) > 1 else 0\n",
    "mean_dobj = statistics.mean(dobj_ttrs) if dobj_ttrs else 0\n",
    "sd_dobj = statistics.stdev(dobj_ttrs) if len(dobj_ttrs) > 1 else 0\n",
    "\n",
    "# Write the results to a CSV file\n",
    "with open(output_csv, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(csv_header)  # Write the header\n",
    "    writer.writerows(results)  # Write the individual file results\n",
    "\n",
    "    # Write summary statistics\n",
    "    writer.writerow(['Mean AMOD TTR', mean_amod])\n",
    "    writer.writerow(['SD AMOD TTR', sd_amod])\n",
    "    writer.writerow(['Mean ADVMOD TTR', mean_advmod])\n",
    "    writer.writerow(['SD ADVMOD TTR', sd_advmod])\n",
    "    writer.writerow(['Mean DOBJ TTR', mean_dobj])\n",
    "    writer.writerow(['SD DOBJ TTR', sd_dobj])\n",
    "\n",
    "print(f\"Phraseological diversity metrics written to {output_csv}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54ab9630-758d-448d-ad3c-83e884084f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/shuyuan/Desktop/CSSMA-master/Corpus linguistics final project'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60654414-76b6-40e3-ba65-3baa48d407ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency frequency list written to dependency_frequencies_A2.csv\n"
     ]
    }
   ],
   "source": [
    "# generat frequency list & calculate raw freq & normed freq\n",
    "import os\n",
    "import csv\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize SpaCy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Path to the folder containing text files\n",
    "folder_path = \"/Users/shuyuan/Desktop/CSSMA-master/Chinese spoken corpus/ICNALE_SM/ICNALE_SM_CHN_A2_0_N056\"\n",
    "\n",
    "# Output CSV file for the frequency list\n",
    "output_csv_freq = \"dependency_frequencies_A2.csv\"\n",
    "\n",
    "# Collect dependencies separately\n",
    "dependencies = {'amod': Counter(), 'advmod': Counter(), 'dobj': Counter()}\n",
    "total_word_count = 0\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):  # Ensure we're only processing .txt files\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Process the text with SpaCy\n",
    "        doc = nlp(text)\n",
    "        total_word_count += len(doc)\n",
    "\n",
    "        # Collect tokens for each relation\n",
    "        for token in doc:\n",
    "            if token.dep_ in dependencies:\n",
    "                dep_pair = (token.head.lemma_, token.lemma_)  # Pair of governor and dependent\n",
    "                dependencies[token.dep_][dep_pair] += 1\n",
    "\n",
    "# Filter out dependencies that occur only once and sort\n",
    "sorted_dependencies = {}\n",
    "for dep, counter in dependencies.items():\n",
    "    sorted_deps = {pair: freq for pair, freq in counter.items() if freq > 1}\n",
    "    sorted_deps = sorted(sorted_deps.items(), key=lambda item: item[1], reverse=True)\n",
    "    sorted_dependencies[dep] = [(pair, freq, (freq / total_word_count) * 100) for pair, freq in sorted_deps]\n",
    "\n",
    "# Determine the maximum length of the lists\n",
    "max_length = max(len(sorted_dependencies[dep]) for dep in sorted_dependencies)\n",
    "\n",
    "# Write the sorted and normalized frequencies to a CSV file\n",
    "with open(output_csv_freq, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    # Write the headers\n",
    "    writer.writerow(['AMOD Dependency', 'AMOD Raw Freq', 'AMOD Norm Freq (per 100 words)',\n",
    "                     'ADVMOD Dependency', 'ADVMOD Raw Freq', 'ADVMOD Norm Freq (per 100 words)',\n",
    "                     'DOBJ Dependency', 'DOBJ Raw Freq', 'DOBJ Norm Freq (per 100 words)'])\n",
    "\n",
    "    # Write the content\n",
    "    for i in range(max_length):\n",
    "        row = []\n",
    "        for dep in ['amod', 'advmod', 'dobj']:\n",
    "            # Check if the index exists for the dependency list\n",
    "            if i < len(sorted_dependencies[dep]):\n",
    "                dep_pair_str = ' - '.join(sorted_dependencies[dep][i][0])  # Convert tuple to string\n",
    "                row.extend([dep_pair_str] + list(sorted_dependencies[dep][i][1:]))\n",
    "            else:\n",
    "                # If there are no more dependencies for this type, add empty values\n",
    "                row.extend(['', '', ''])\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Dependency frequency list written to {output_csv_freq}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "659a8326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency frequency list written to dependency_frequencies_B1_2.csv\n"
     ]
    }
   ],
   "source": [
    "# updated version with the large spacy, USE THIS ONE!!\n",
    "\n",
    "\n",
    "import csv\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize SpaCy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Path to the file containing the text\n",
    "file_path = \"/Users/shuyuan/Desktop/CSSMA-master/Chinese spoken corpus/ICNALE_SM/SM_CHN_B1_2.txt\"\n",
    "\n",
    "# Output CSV file for the frequency list\n",
    "output_csv_freq = \"dependency_frequencies_B1_2.csv\"\n",
    "\n",
    "# Collect dependencies separately\n",
    "dependencies = {'amod': Counter(), 'advmod': Counter(), 'dobj': Counter()}\n",
    "\n",
    "# Read the text file\n",
    "with open(file_path, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Process the text with SpaCy\n",
    "doc = nlp(text)\n",
    "total_word_count = len(doc)\n",
    "\n",
    "# Collect tokens for each relation\n",
    "for token in doc:\n",
    "    if token.dep_ == 'amod' and token.head.pos_ == 'NOUN':\n",
    "        dependencies['amod'][f\"{token.text} {token.head.text}\"] += 1\n",
    "    elif token.dep_ == 'advmod' and (token.head.pos_ == 'ADJ' or token.head.pos_ == 'VERB'):\n",
    "        dependencies['advmod'][f\"{token.text} {token.head.text}\"] += 1\n",
    "    elif token.dep_ == 'dobj':\n",
    "        dependencies['dobj'][f\"{token.head.text} + {token.text}\"] += 1\n",
    "\n",
    "# Filter out dependencies that occur only once and sort\n",
    "sorted_dependencies = {}\n",
    "for dep, counter in dependencies.items():\n",
    "    sorted_deps = {pair: freq for pair, freq in counter.items() if freq > 1}\n",
    "    sorted_deps = sorted(sorted_deps.items(), key=lambda item: item[1], reverse=True)\n",
    "    sorted_dependencies[dep] = [(pair, freq, (freq / total_word_count) * 100) for pair, freq in sorted_deps]\n",
    "\n",
    "# Write the sorted and normalized frequencies to a CSV file\n",
    "with open(output_csv_freq, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    # Write the headers\n",
    "    writer.writerow(['AMOD Pair', 'AMOD Raw Freq', 'AMOD Norm Freq (per 100 words)',\n",
    "                     'ADVMOD Pair', 'ADVMOD Raw Freq', 'ADVMOD Norm Freq (per 100 words)',\n",
    "                     'DOBJ Pair', 'DOBJ Raw Freq', 'DOBJ Norm Freq (per 100 words)'])\n",
    "\n",
    "    # Get the max length of dependency lists\n",
    "    max_length = max(len(sorted_dependencies[dep]) for dep in sorted_dependencies)\n",
    "\n",
    "    # Write the content\n",
    "    for i in range(max_length):\n",
    "        row = []\n",
    "        for dep in ['amod', 'advmod', 'dobj']:\n",
    "            if i < len(sorted_dependencies[dep]):\n",
    "                # Write the pair, raw frequency, and normalized frequency\n",
    "                row.extend([sorted_dependencies[dep][i][0], sorted_dependencies[dep][i][1], sorted_dependencies[dep][i][2]])\n",
    "            else:\n",
    "                # If there are no more dependencies for this type, add empty values\n",
    "                row.extend(['', '', ''])\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Dependency frequency list written to {output_csv_freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec235588-9a30-405e-a64f-9be9964bdd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
