{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67dd37bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.4-cp39-cp39-macosx_10_9_x86_64.whl (6.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy) (4.64.1)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2\n",
      "  Downloading thinc-8.2.3-cp39-cp39-macosx_10_9_x86_64.whl (880 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.3/880.3 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp39-cp39-macosx_10_9_x86_64.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp39-cp39-macosx_10_9_x86_64.whl (493 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.1/493.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.6.4-py3-none-any.whl (394 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp39-cp39-macosx_10_9_x86_64.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.28.1)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp39-cp39-macosx_10_9_x86_64.whl (26 kB)\n",
      "Requirement already satisfied: setuptools in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy) (63.4.1)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Collecting typing-extensions>=4.6.1\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting pydantic-core==2.16.3\n",
      "  Downloading pydantic_core-2.16.3-cp39-cp39-macosx_10_12_x86_64.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp39-cp39-macosx_10_9_x86_64.whl (6.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: cymem, wasabi, typing-extensions, spacy-loggers, spacy-legacy, murmurhash, langcodes, catalogue, blis, annotated-types, typer, srsly, pydantic-core, preshed, cloudpathlib, pydantic, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.6.4 pydantic-core-2.16.3 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.4 typing-extensions-4.10.0 wasabi-1.1.2 weasel-0.3.4\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc2cd16-52f8-47d1-b5d6-7f7e0a99d9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-lg==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: setuptools in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (63.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.28.1)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.11.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.21.5)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.6.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.64.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2022.9.24)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/shuyuan/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.1)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b72653-055f-402f-8737-9571723c015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  step 1: processing the textx: lemma, pos tag, and parse the reference native speaker corpus\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model for English\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Define the path to your text file\n",
    "file_path = '/Users/shuyuan/Desktop/CSSMA-master/Chinese spoken corpus/sophistication/SM_ENS.txt'\n",
    "\n",
    "# Process the text file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Define the name of the output file\n",
    "output_file_path = file_path.replace('.txt', '_processed.txt')\n",
    "\n",
    "# Open the output file and write the lemmatized form, POS tag, and dependency parse\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    for token in doc:\n",
    "        output_file.write(f\"{token.text}\\t{token.lemma_}\\t{token.pos_}\\t{token.dep_}\\n\")\n",
    "\n",
    "# Note: This will create a file named `file_processed.txt` with the processed information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62b88c76-6a23-4769-b35b-6fd7063fa13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/shuyuan/Desktop/CSSMA-master/Corpus linguistics final project'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f065cad-af0c-4788-b059-e9a371a2e05e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '/Users/shuyuan/Desktop/CSSMA-master/Chinese spoken corpus/sophistication/SM_ENS.txt/reference corpus_dependecy list.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fn/dvg_ws213_l4wv2rh5gy5jjm0000gn/T/ipykernel_46108/3275361042.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# Write to CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mcsvwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/Users/shuyuan/Desktop/CSSMA-master/Chinese spoken corpus/sophistication/SM_ENS.txt/reference corpus_dependecy list.csv'"
     ]
    }
   ],
   "source": [
    "# step 2: extract the dependecies, raw freq, and normed freq\n",
    "\n",
    "import spacy\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Initialize counters for each dependency type\n",
    "dep_counters = {\n",
    "    'amod': Counter(),\n",
    "    'advmod': Counter(),\n",
    "    'dobj': Counter()\n",
    "}\n",
    "\n",
    "# Process the text file\n",
    "input_file_path = '/Users/shuyuan/Desktop/CSSMA-master/Chinese spoken corpus/sophistication/SM_ENS.txt'\n",
    "output_csv_path = '/Users/shuyuan/Desktop/CSSMA-master/Chinese spoken corpus/sophistication/SM_ENS.txt/reference corpus_dependecy list.csv'\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "    text = input_file.read()\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Count dependencies\n",
    "    for token in doc:\n",
    "        if token.dep_ in dep_counters:\n",
    "            dep_counters[token.dep_][(token.head.text, token.text)] += 1\n",
    "\n",
    "# Calculate normalized frequencies\n",
    "norm_factors = {'amod': 1000, 'advmod': 1000, 'dobj': 1000}  # Example normalization factors\n",
    "normed_freqs = {\n",
    "    dep: {pair: (count / sum(counter.values()) * norm_factors[dep])\n",
    "          for pair, count in counter.items()}\n",
    "    for dep, counter in dep_counters.items()\n",
    "}\n",
    "\n",
    "# Prepare data for CSV output, including raw and normed frequencies\n",
    "rows = []\n",
    "max_len = max(len(counter) for counter in dep_counters.values())\n",
    "for i in range(max_len):\n",
    "    row = []\n",
    "    for dep, counter in dep_counters.items():\n",
    "        if i < len(counter):\n",
    "            pair, raw_freq = counter.most_common()[i]\n",
    "            # Adjust formatting based on dependency type, taking the new definition into account\n",
    "            if dep == 'amod':  # Adjectival modifier: adjective + noun\n",
    "                formatted_pair = f\"{pair[1]} {pair[0]}\"\n",
    "            elif dep == 'advmod':  # Adverbial modifier: adverb + adjective or adverb\n",
    "                formatted_pair = f\"{pair[1]} {pair[0]}\"\n",
    "            elif dep == 'dobj':  # Direct object: verb + object\n",
    "                formatted_pair = f\"{pair[0]}+{pair[1]}\"\n",
    "            # Retrieve the normalized frequency of the word pair\n",
    "            normed_freq = normed_freqs[dep][pair]\n",
    "            # Extend the current row with the formatted word pair, raw frequency, and normalized frequency\n",
    "            row.extend([formatted_pair, raw_freq, f\"{normed_freq:.2f}\"])\n",
    "        else:\n",
    "            # Fill the row with placeholders if there's no data for the current index\n",
    "            row.extend([\"\", \"\", \"\"])\n",
    "    # Append the completed row to the list of rows\n",
    "    rows.append(row)\n",
    "\n",
    "\n",
    "# The rest of the CSV writing process remains the same\n",
    "\n",
    "\n",
    "\n",
    "# Write to CSV\n",
    "with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    headers = []\n",
    "    for dep in ['AMOD', 'ADVMOD', 'DOBJ']:\n",
    "        headers.extend([f\"{dep} Pair\", f\"{dep} Raw Freq\", f\"{dep} Normed Freq\"])\n",
    "    csvwriter.writerow(headers)  # Header\n",
    "    csvwriter.writerows(rows)\n",
    "\n",
    "print(\"CSV file with raw and normed frequencies has been created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff0a0d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file with structured dependencies has been created.\n"
     ]
    }
   ],
   "source": [
    "# extract dependencies, freq, and normed freq from reference corpus\n",
    "import spacy\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Initialize counters for each type of dependency pair\n",
    "amod_counter = Counter()\n",
    "advmod_counter = Counter()\n",
    "dobj_counter = Counter()\n",
    "\n",
    "# Process the text file\n",
    "\n",
    "input_file_path = '/Users/shuyuan/Desktop/CSSMA-master/Chinese spoken corpus/sophistication/SM_ENS.txt'\n",
    "output_csv_path = '/Users/shuyuan/Desktop/CSSMA-master/Chinese spoken corpus/sophistication/reference_SM.csv'\n",
    "\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "    text = input_file.read()\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Iterate through tokens in the document\n",
    "    for token in doc:\n",
    "        # For AMOD: Adjective + Noun\n",
    "        if token.dep_ == 'amod' and token.head.pos_ == 'NOUN':\n",
    "            amod_counter[(token.text, token.head.text)] += 1\n",
    "        \n",
    "        # For ADVMOD: Adverb + Adjective/Adverb/Noun\n",
    "        elif token.dep_ == 'advmod':\n",
    "            if token.head.pos_ in ['ADJ', 'ADV', 'NOUN']:\n",
    "                advmod_counter[(token.text, token.head.text)] += 1\n",
    "        \n",
    "        # For DOBJ: Verb + Direct Object\n",
    "        elif token.dep_ == 'dobj' and token.head.pos_ == 'VERB':\n",
    "            dobj_counter[(token.head.text, token.text)] += 1\n",
    "\n",
    "# Write the CSV file with specified headers\n",
    "with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    \n",
    "    # Define the headers\n",
    "    headers = [\n",
    "        \"AMOD Pair\", \"AMOD Raw Freq\", \"AMOD Normed Freq\",\n",
    "        \"ADVMOD Pair\", \"ADVMOD Raw Freq\", \"ADVMOD Normed Freq\",\n",
    "        \"DOBJ Pair\", \"DOBJ Raw Freq\", \"DOBJ Normed Freq\"\n",
    "    ]\n",
    "    csvwriter.writerow(headers)\n",
    "    \n",
    "    # Calculate normalization factor as the sum of all counts for each dependency type\n",
    "    amod_norm_factor = 1000 / sum(amod_counter.values())\n",
    "    advmod_norm_factor = 1000 / sum(advmod_counter.values())\n",
    "    dobj_norm_factor = 1000 / sum(dobj_counter.values())\n",
    "\n",
    "    # Assuming the same number of entries for each type, iterate and write to CSV\n",
    "    for ((amod_pair, amod_freq), (advmod_pair, advmod_freq), (dobj_pair, dobj_freq)) in zip(\n",
    "        amod_counter.most_common(), advmod_counter.most_common(), dobj_counter.most_common()):\n",
    "\n",
    "        # Normalize frequencies\n",
    "        amod_norm = amod_freq * amod_norm_factor\n",
    "        advmod_norm = advmod_freq * advmod_norm_factor\n",
    "        dobj_norm = dobj_freq * dobj_norm_factor\n",
    "        \n",
    "        # Write the rows as specified\n",
    "        csvwriter.writerow([\n",
    "            ' '.join(amod_pair), amod_freq, f\"{amod_norm:.2f}\",\n",
    "            ' '.join(advmod_pair), advmod_freq, f\"{advmod_norm:.2f}\",\n",
    "            ' '.join(dobj_pair), dobj_freq, f\"{dobj_norm:.2f}\"\n",
    "        ])\n",
    "\n",
    "print(\"CSV file with structured dependencies has been created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8608e252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file with MI scores has been created: /Users/shuyuan/Desktop/CSSMA-master/Chinese spoken corpus/sophistication/reference_SM_MI.csv\n"
     ]
    }
   ],
   "source": [
    "# calculating the MI score in the reference corpus-updated\n",
    "\n",
    "import csv\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# Make sure to use the correct path after you upload the CSV to the server\n",
    "\n",
    "input_csv_path = '/Users/shuyuan/Desktop/CSSMA-master/Chinese spoken corpus/sophistication/reference_SM.csv'\n",
    "output_csv_path = '/Users/shuyuan/Desktop/CSSMA-master/Chinese spoken corpus/sophistication/reference_SM_MI.csv'\n",
    "\n",
    "\n",
    "# Initialize counters and variables for MI calculation\n",
    "word_counts = defaultdict(int)\n",
    "pair_counts = defaultdict(int)\n",
    "total_count = 0\n",
    "\n",
    "# Process the CSV and calculate word and pair counts\n",
    "data_rows = []\n",
    "with open(input_csv_path, 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    # Store the data in memory to use after the file is closed\n",
    "    data_rows = [row for row in reader]\n",
    "    for row in data_rows:\n",
    "        for dep_type in ['AMOD', 'ADVMOD', 'DOBJ']:\n",
    "            pair = row[f\"{dep_type} Pair\"]\n",
    "            if pair:  # Check if the pair exists\n",
    "                raw_freq = int(row[f\"{dep_type} Raw Freq\"])\n",
    "                word1, word2 = pair.split()\n",
    "                pair_counts[(dep_type, word1, word2)] += raw_freq\n",
    "                word_counts[word1] += raw_freq\n",
    "                word_counts[word2] += raw_freq\n",
    "                total_count += raw_freq\n",
    "\n",
    "# Calculate PMI scores for each dependency type\n",
    "pmi_scores = {}\n",
    "for (dep_type, word1, word2), pair_freq in pair_counts.items():\n",
    "    p_word1 = word_counts[word1] / total_count\n",
    "    p_word2 = word_counts[word2] / total_count\n",
    "    p_pair = pair_freq / total_count\n",
    "    pmi = math.log2(p_pair / (p_word1 * p_word2))\n",
    "    pmi_scores[(dep_type, word1, word2)] = pmi\n",
    "\n",
    "# Write the new CSV file with MI scores\n",
    "with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = reader.fieldnames + ['AMOD MI Score', 'ADVMOD MI Score', 'DOBJ MI Score']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Iterate over the stored data\n",
    "    for row in data_rows:\n",
    "        new_row = row.copy()  # Work with a copy of the row\n",
    "        for dep_type in ['AMOD', 'ADVMOD', 'DOBJ']:\n",
    "            pair = row[f\"{dep_type} Pair\"]\n",
    "            if pair:\n",
    "                word1, word2 = pair.split()\n",
    "                # Get the PMI score from the calculated values\n",
    "                pmi_score = pmi_scores.get((dep_type, word1, word2))\n",
    "                new_row[f\"{dep_type} MI Score\"] = f\"{pmi_score:.4f}\" if pmi_score is not None else \"\"\n",
    "        writer.writerow(new_row)\n",
    "\n",
    "print(f\"CSV file with MI scores has been created: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d323cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the files by adding the MI scores to each proficiency level\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the two CSV files\n",
    "MIscore_proficiency = pd.read_csv('/Users/shuyuan/Desktop/CSSMA-master/Chinese spoken corpus/sophistication/MI score/DOBJ/DOBJ_B1_2_cleaned_1.csv')\n",
    "reference_scores = pd.read_csv('/Users/shuyuan/Desktop/CSSMA-master/Chinese spoken corpus/sophistication/reference_SM_MI.csv')\n",
    "\n",
    "# Assuming 'student_id' is the common column in both CSV files\n",
    "# and 'reference_score' is the column name in the reference scores file that you want to add\n",
    "merged_df = pd.merge(MIscore_proficiency, reference_scores[['DOBJ Pair', 'DOBJ MI Score']], on='DOBJ Pair', how='left')\n",
    "\n",
    "# Export the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('DOBJ_B1_2_MI.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23f71ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been processed and saved to: /Users/shuyuan/Desktop/CSSMA-master/Chinese spoken corpus/sophistication/MI score/DOBJ/DOBJ_B1_2_cleaned_1.csv\n"
     ]
    }
   ],
   "source": [
    "# data cleaning: replace all the \" + \" with \"+\" to find the matched DOBJ\n",
    "\n",
    "import csv\n",
    "\n",
    "# Specify the path to your input and output CSV files\n",
    "input_csv_path = '/Users/shuyuan/Desktop/CSSMA-master/Chinese spoken corpus/sophistication/MI score/DOBJ/DOBJ_B1_2_cleaned.csv'\n",
    "output_csv_path = '/Users/shuyuan/Desktop/CSSMA-master/Chinese spoken corpus/sophistication/MI score/DOBJ/DOBJ_B1_2_cleaned_1.csv'\n",
    "\n",
    "# Open the input CSV file for reading\n",
    "with open(input_csv_path, mode='r', encoding='utf-8') as infile:\n",
    "    # Open the output CSV file for writing\n",
    "    with open(output_csv_path, mode='w', encoding='utf-8', newline='') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "        \n",
    "        # Iterate through each row in the input CSV\n",
    "        for row in reader:\n",
    "            # Check if the row is not empty\n",
    "            if row:\n",
    "                # Replace \" + \" with \"+\" in the first column\n",
    "                row[0] = row[0].replace(\"+\", \" \")\n",
    "            # Write the modified row to the output CSV\n",
    "            writer.writerow(row)\n",
    "\n",
    "print(\"The file has been processed and saved to:\", output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a61bf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file with modified 'DOBJ Pair' column has been created: /Users/shuyuan/Desktop/CSSMA-master/Corpus linguistics final project/native speaker reference corpus/reference_corpus_MI_score_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# add '+' to clean the dobj data in reference corpus\n",
    "\n",
    "import csv\n",
    "\n",
    "# Input and output file paths\n",
    "input_csv_path = '/Users/shuyuan/Desktop/CSSMA-master/Corpus linguistics final project/native speaker reference corpus/reference_corpus_MI_score.csv'\n",
    "output_csv_path = '/Users/shuyuan/Desktop/CSSMA-master/Corpus linguistics final project/native speaker reference corpus/reference_corpus_MI_score_cleaned.csv'\n",
    "\n",
    "# Read the CSV, modify the \"DOBJ Pair\" column, and write to a new CSV\n",
    "with open(input_csv_path, 'r', encoding='utf-8') as input_csvfile, \\\n",
    "     open(output_csv_path, 'w', newline='', encoding='utf-8') as output_csvfile:\n",
    "    \n",
    "    reader = csv.DictReader(input_csvfile)\n",
    "    writer = csv.DictWriter(output_csvfile, fieldnames=reader.fieldnames)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    for row in reader:\n",
    "        # Modify the \"DOBJ Pair\" by adding \"+\" between the words\n",
    "        if row['DOBJ Pair']:\n",
    "            words = row['DOBJ Pair'].split()\n",
    "            row['DOBJ Pair'] = '+'.join(words)\n",
    "        \n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"CSV file with modified 'DOBJ Pair' column has been created: {output_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
